{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe59250",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "## Final Project\n",
    "\n",
    "Lokesh Roopkumar\n",
    "lroopk2@uic.edu\n",
    "\n",
    "Please contact via email if there are any issues while executing the code.\n",
    "\n",
    "Thank you so much for your time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b839a1",
   "metadata": {},
   "source": [
    "### Part 1 - Web Crawler\n",
    "\n",
    "Please note that the crawler takes a long time to run. To give you an estimate, it took 1 hour and 36 minutes to crawl 4000 pages. \n",
    "\n",
    "The crawled data already exists in the CrawledPages directory (for 4000 pages). So the cell below need not be run for the project to work. Running the cell below might even destroy the existing file that has the crawled pages results. So please make a copy of that file before running the crawler again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "47ec3947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not connect to https://securityigert.uic.edu/ or https://www.uis.edu/\n",
      "Error occurred:  ('Connection aborted.', FileNotFoundError(2, 'No such file or directory')) \n",
      "\n",
      "Could not connect to https://www.cmhsrp.uic.edu/nrtc/ or https://dscc.uic.edu/terms-of-use/\n",
      "Error occurred:  ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokes\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not connect to https://hpsc.org.uic.edu/ or https://www.uis.edu/\n",
      "Error occurred:  ('Connection aborted.', FileNotFoundError(2, 'No such file or directory')) \n",
      "\n",
      "Could not connect to https://nlp.cs.uic.edu/ or https://www.uis.edu/\n",
      "Error occurred:  HTTPSConnectionPool(host='nlp.cs.uic.edu', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000231363C63D0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')) \n",
      "\n",
      "Could not connect to https://projectheal.uic.edu/ or https://www.uis.edu/\n",
      "Error occurred:  HTTPSConnectionPool(host='projectheal.uic.edu', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002312F3A6E50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')) \n",
      "\n",
      "Could not connect to https://tigger.uic.edu/htbin/codewrap/bin/orgs/ura/cgi-bin/ or https://www.uis.edu/\n",
      "Error occurred:  HTTPSConnectionPool(host='tigger.uic.edu', port=443): Max retries exceeded with url: /htbin/codewrap/bin/orgs/ura/cgi-bin/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000023134FA40D0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')) \n",
      "\n",
      "Could not connect to https://www.ecc.uic.edu/ecc/webhome/ or https://www.vpaa.uillinois.edu/resources/web_privacy/\n",
      "Error occurred:  HTTPSConnectionPool(host='ecc.engr.uic.edu', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000023136DFA610>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')) \n",
      "\n",
      "Could not connect to https://www.uic.edu/depts/oaa/wisest/ or https://www.uis.edu/\n",
      "Error occurred:  HTTPConnectionPool(host='wisest.uic.edu', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023136D7FD90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')) \n",
      "\n",
      "Could not connect to https://cts.cs.uic.edu/ or https://www.acm.org/sigkdd/\n",
      "Error occurred:  HTTPSConnectionPool(host='cts.cs.uic.edu', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000023138705190>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')) \n",
      "\n",
      "Could not connect to https://gila.bioe.uic.edu/lab/ or https://www.uis.edu/\n",
      "Error occurred:  HTTPSConnectionPool(host='gila.bioe.uic.edu', port=443): Max retries exceeded with url: /lab/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002313999A220>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')) \n",
      "\n",
      "Could not connect to https://sts.bioe.uic.edu/castp/ or https://www.uis.edu/\n",
      "Error occurred:  HTTPSConnectionPool(host='sts.bioe.uic.edu', port=443): Max retries exceeded with url: /castp/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002313999AA00>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')) \n",
      "\n",
      "Could not connect to https://healthcheck.uic.edu/ or https://uis.edu/\n",
      "Error occurred:  HTTPSConnectionPool(host='healthcheck.uic.edu', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002313996D2B0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')) \n",
      "\n",
      "Could not connect to https://go.uic.edu/dailypass/ or \n",
      "Error occurred:  HTTPSConnectionPool(host='healthcheck.uic.edu', port=443): Max retries exceeded with url: /AAMEmployeeOnsiteStatus/Home/CheckStatus/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002313A697670>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')) \n",
      "\n",
      "Could not connect to https://go.uic.edu/dailysurvey/ or https://www.uis.edu/\n",
      "Error occurred:  HTTPSConnectionPool(host='healthcheck.uic.edu', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002313C76BCD0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')) \n",
      "\n",
      "Crawled 4000 pages in 96.04406913121541 seconds\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup   \n",
    "from bs4.element import Comment\n",
    "import requests                                                             \n",
    "requests.packages.urllib3.disable_warnings()\n",
    "from collections import defaultdict, deque\n",
    "import time\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from os import makedirs\n",
    "\n",
    "headers = {\n",
    "  'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Mobile Safari/537.36'\n",
    "}\n",
    "\n",
    "pages_folder = './CrawledPages/'                                            \n",
    "makedirs(pages_folder, exist_ok=True)\n",
    "crawl_limit = 4000                                                          \n",
    "domain = 'uic.edu'                                                          \n",
    "seed_url = 'https://cs.uic.edu/'\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# file to store error logs\n",
    "\n",
    "error_file = 'error_logs.txt'\n",
    "file_writer = open(error_file, 'w+')\n",
    "file_writer.close()\n",
    "\n",
    "# filters unnecessary extensions\n",
    "\n",
    "def extension_found(url):\n",
    "    ignore_ext = ['.css', '.js', '.aspx', '.pdf', '.doc', '.docx', '.ppt', '.pptx', '.xls', '.xlsx', '.tar', '.gz', '.tgz', '.zip', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.mp4', '.avi', '.ics', '/googlepublish', '/ical', '.xml']\n",
    "    for ext in ignore_ext:\n",
    "        if ext in url:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ensures information from certain tags are ignored\n",
    "\n",
    "def filter_tags(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'meta', '[document]']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):\n",
    "        return False\n",
    "    elif re.match(r'[\\s\\r\\n]+', str(element)):\n",
    "        return False\n",
    "    elif 'encoding=\\\"utf-8\\\"' in element.lower():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "urls_visited = set()\n",
    "bfs_queue = deque()                                                         \n",
    "bfs_queue.append(seed_url)                                                  \n",
    "\n",
    "url_content = dict()                                                        \n",
    "num_pages_crawled = 0\n",
    "\n",
    "# timer is started to track how long it takes to crawl\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# the main web crawling part\n",
    "\n",
    "while bfs_queue:\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        curr_url = bfs_queue.popleft()                                          # urls are popped from the queue\n",
    "        urls_visited.add(curr_url)\n",
    "        get_request = requests.get(curr_url, headers = headers, verify=False)   # get request is made\n",
    "        if get_request.status_code == 200:\n",
    "            soup = BeautifulSoup(get_request.text, 'lxml')                      # Beautiful Soup is used to parse html\n",
    "            soup_text = soup.find_all(text=True)\n",
    "            \n",
    "            visible_text = filter(filter_tags, soup_text)                        # filtering out text in unnecessary tags\n",
    "            visible_text = ' '.join(term.strip() for term in visible_text)\n",
    "            visible_text = visible_text.lower()\n",
    "            visible_text = re.sub('[^a-z]+', ' ', visible_text)                  # text cleaning\n",
    "            tokens = visible_text.split()                                  \n",
    "            \n",
    "            clean_tokens = list()                                 \n",
    "            for token in tokens:\n",
    "                if token not in stop_words:                                      # tokenize, stem and remove stopwords\n",
    "                    temp_token = stemmer.stem(token)\n",
    "                    if temp_token not in stop_words and len(temp_token) > 1:\n",
    "                        clean_tokens.append(temp_token)\n",
    "            url_content[curr_url] = clean_tokens\n",
    "\n",
    "            for link in soup.find_all('a'):                                      # extract all links\n",
    "                curr_outgoing_link = link.get('href')\n",
    "\n",
    "                if curr_outgoing_link:\n",
    "                    curr_outgoing_link = curr_outgoing_link.strip()              # clean and normalize urls\n",
    "                    curr_outgoing_link = curr_outgoing_link.lower()                       \n",
    "                    curr_outgoing_link = curr_outgoing_link.replace('http://', 'https://')\n",
    "                    curr_outgoing_link = curr_outgoing_link.split(\"#\")[0]\n",
    "                    curr_outgoing_link = curr_outgoing_link.split(\"?\", maxsplit=1)[0]\n",
    "\n",
    "                    if len(curr_outgoing_link) > 0 and curr_outgoing_link[-1] != '/':\n",
    "                        curr_outgoing_link += '/'\n",
    "\n",
    "                    if len(curr_outgoing_link) > 0 and curr_outgoing_link[0] == '/':     # expanding relative urls\n",
    "                        index = curr_url.find('.edu')\n",
    "                        index += 5\n",
    "                        curr_outgoing_link = curr_url[:index] + curr_outgoing_link[1:]\n",
    "\n",
    "                    if domain in curr_outgoing_link and 'https' in curr_outgoing_link and curr_outgoing_link not in bfs_queue and curr_outgoing_link not in urls_visited:\n",
    "                        if extension_found(curr_outgoing_link) == False:\n",
    "                            bfs_queue.append(curr_outgoing_link)\n",
    "\n",
    "        num_pages_crawled += 1\n",
    "        if num_pages_crawled == crawl_limit:\n",
    "            break\n",
    "    \n",
    "    except Exception as e:                                               # handling exceptions and writing to error logs file\n",
    "        with open(error_file, 'a+') as file_writer:\n",
    "            file_writer.write(f'Error either with {curr_url} or {curr_outgoing_link}')\n",
    "            file_writer.write(f'\\nError: {e}\\n\\n')\n",
    "            \n",
    "            print(f'Could not connect to {curr_url} or {curr_outgoing_link}')\n",
    "            print(f'Error occurred: ', e, '\\n')\n",
    "            continue\n",
    "            \n",
    "# the urls are their corresponding tokens are stashed in a json file\n",
    "            \n",
    "with open(pages_folder + '4000_crawled_pages.json', 'w') as fw:\n",
    "    json.dump(url_content, fw)\n",
    "        \n",
    "    \n",
    "print(f'Crawled {num_pages_crawled} pages in {(time.time() - start_time)/60} minutes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ceae9",
   "metadata": {},
   "source": [
    "### Part 2 - Cosine Similarity\n",
    "\n",
    "Just running the cell below would suffice. This is where I read in all the stored crawled pages data and calculate cosine similarity. Please ensure that there is a folder called CrawledPages in the same directory with a file inside it called 4000_crawled_pages.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c40360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a search query: covid\n",
      "\n",
      "Search Results: \n",
      "1 https://today.uic.edu/coronavirus/\n",
      "2 https://ehso.uic.edu/covid19/\n",
      "3 https://today.uic.edu/uic-fall-covid-19-guidance/\n",
      "4 https://today.uic.edu/covid-19-vaccination-guidelines/\n",
      "5 https://law.uic.edu/coronavirus/\n",
      "6 https://dos.uic.edu/community-standards/uic-covid-19-guidance/\n",
      "7 https://today.uic.edu/frequently-asked-questions/\n",
      "8 https://research.uic.edu/covid-19ovcr/\n",
      "9 https://today.uic.edu/updates-to-covid-19-university-sponsored-travel-approvals/\n",
      "10 https://vcha.uic.edu/about/vcha-initiatives/covid-19-university-travel-request-authorization-form/\n"
     ]
    }
   ],
   "source": [
    "from json import load\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from math import log2\n",
    "from collections import Counter\n",
    "from re import sub\n",
    "from math import sqrt\n",
    "\n",
    "crawled_folder_path = './CrawledPages/'\n",
    "\n",
    "with open(crawled_folder_path + '4000_crawled_pages.json', 'r') as fr:\n",
    "    url_tokens = load(fr)\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "url_clean_tokens = dict()\n",
    "index_urls = dict()\n",
    "most_freq_url = dict()\n",
    "\n",
    "# remove urls that contain less than 10 words\n",
    "\n",
    "for url_key in url_tokens:\n",
    "    if len(url_tokens[url_key]) > 10:\n",
    "        url_clean_tokens[url_key] = url_tokens[url_key]\n",
    "\n",
    "# find frequency of most occuring word in every url\n",
    "        \n",
    "for url_key in url_clean_tokens:\n",
    "    most_freq_url[url_key] = Counter(url_clean_tokens[url_key]).most_common(1)[0][1]\n",
    "\n",
    "# construct inverted index for the urls\n",
    "\n",
    "for url_key in url_clean_tokens:\n",
    "    for word in url_clean_tokens[url_key]:\n",
    "        if word in index_urls:\n",
    "            if url_key in index_urls[word]:\n",
    "                index_urls[word][url_key] += 1\n",
    "            else:\n",
    "                index_urls[word][url_key] = 1\n",
    "        else:\n",
    "            index_urls[word] = dict()\n",
    "            index_urls[word][url_key] = 1\n",
    "\n",
    "# compute weights for every word in every url and store them\n",
    "            \n",
    "weights_urls = dict()\n",
    "for word in index_urls:\n",
    "    idf = round( log2(len(url_clean_tokens) / len(index_urls[word])), 8)\n",
    "    for every_url in index_urls[word]:\n",
    "        if every_url in weights_urls:\n",
    "            weights_urls[every_url][word] = round( (index_urls[word][every_url] / most_freq_url[every_url]) * idf, 8)\n",
    "        else:\n",
    "            weights_urls[every_url] = dict()\n",
    "            weights_urls[every_url][word] = round( (index_urls[word][every_url] / most_freq_url[every_url]) * idf, 8)\n",
    "            \n",
    "# take the query as input, tokenize, stem and remove stop words i.e. input query cleaning\n",
    "            \n",
    "print()\n",
    "input_query = input('Enter a search query: ')\n",
    "print()\n",
    "clean_query = input_query.lower()\n",
    "clean_query = sub('[^a-z]+', ' ', clean_query)\n",
    "query_tokens = clean_query.split()\n",
    "clean_query_tokens = list()\n",
    "for token in query_tokens:\n",
    "    if token not in nltk_stopwords:\n",
    "        stemmed_token = porter_stemmer.stem(token)\n",
    "        if stemmed_token not in nltk_stopwords and len(stemmed_token) > 1:\n",
    "            clean_query_tokens.append(stemmed_token)\n",
    "\n",
    "# construct query term frequencies index\n",
    "\n",
    "index_query = dict()\n",
    "for token in clean_query_tokens:\n",
    "    if token in index_query:\n",
    "        index_query[token] += 1\n",
    "    else:\n",
    "        index_query[token] = 1\n",
    "\n",
    "# find frequency of most occuring term in the query\n",
    "\n",
    "query_high_freq = Counter(clean_query_tokens).most_common(1)[0][1]\n",
    "\n",
    "# compute weights for every word in every query and store them\n",
    "\n",
    "weights_query = dict()\n",
    "for word in index_query:\n",
    "    if word in index_urls:\n",
    "        idf = round( log2(len(url_clean_tokens) / len(index_urls[word])), 8)\n",
    "    else:\n",
    "        idf = 0\n",
    "    weights_query[word] = round( (index_query[word] / query_high_freq) * idf, 8)\n",
    "    \n",
    "# query vector length for cosine similarity\n",
    "query_length = 0\n",
    "for term in weights_query:\n",
    "    query_length += round( weights_query[term] ** 2, 8)\n",
    "    \n",
    "# url vector lengths for cosine similarity\n",
    "\n",
    "url_lengths = dict()\n",
    "for each_url in weights_urls:\n",
    "    url_size = 0\n",
    "    for term in weights_urls[each_url]:\n",
    "        url_size += round( weights_urls[each_url][term] ** 2, 8)\n",
    "    url_lengths[each_url] = url_size\n",
    "    \n",
    "# cosine similarity\n",
    "\n",
    "url_cosine_sim = dict()\n",
    "for url in weights_urls:\n",
    "    numerator = 0\n",
    "    for term in weights_query:\n",
    "        if term in weights_urls[url]:\n",
    "            numerator += round( weights_query[term] * weights_urls[url][term], 8)\n",
    "    denominator = round( sqrt(query_length * url_lengths[url]), 8)\n",
    "    final_val = round(numerator / denominator, 8)\n",
    "    if final_val > 0:\n",
    "        url_cosine_sim[url] = final_val\n",
    "\n",
    "urls_cosine_sorted = dict(sorted(url_cosine_sim.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# UI\n",
    "\n",
    "print('Search Results: ')\n",
    "i = 1\n",
    "print_limit = 10\n",
    "for url in urls_cosine_sorted:\n",
    "    print(f'{i} {url}')\n",
    "    if i == print_limit:\n",
    "        input_cont = input('Do you want to continue (y/n): ')\n",
    "        if input_cont.lower() == 'y':\n",
    "            print_limit += 10\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ab8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4b84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
